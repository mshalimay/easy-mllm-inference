{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just some tricks to look like we are running comands from the root directory\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to system path\n",
    "root_dir = Path(os.getcwd()).parent\n",
    "# Absolute path\n",
    "root_dir = root_dir.resolve()\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "# Change the working directory to the project root\n",
    "os.chdir(root_dir)\n",
    "print(\"Working directory set to:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Overview\n",
    "\n",
    "The idea of th `llms` package is to make it very simple to send inference to LLMs. \n",
    "In summary: it allows inference calls to any of the supported providers and engines with something as simple as: `messages=list[images, text, video, function_call, ...]` and `generation_configs = {k:v, k:v,..}`. \n",
    "\n",
    "\n",
    "And behind the scenes, the package handles:\n",
    "- Formatting of prompts for providers and models.\n",
    "- Setting up clients, inference engines for HuggingFace\n",
    "- Routing the inference call to the appropriate provider and engine\n",
    "- Load balancing API keys\n",
    "- Retry with exponential backoff with customized error logic\n",
    "- Logging of inputs and outputs, including: HTML visualization of the prompt for debugging; token counts; conversation logs\n",
    "- Return of multimodal outputs in a unified format independent of the provider, model, or provider mode.\n",
    "- Type validation and handling of multiple media types\n",
    "- Etc, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Setting API keys\n",
    "\n",
    "Two alternatives:\n",
    "\n",
    "#### 1) Set the API keys in the environment variables.\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"<key >\"\n",
    "export GOOGLE_API_KEY=\"<key>\"\n",
    "export HF_TOKEN=\"<key>\n",
    "```\n",
    "In this option, **no load balancing** of the keys is performed in case of multiple calls.\n",
    "\n",
    "#### 2)(Recommended) Create a `api_keys.json` file  as follows:\n",
    "\n",
    "api_keys.json\n",
    "```json\n",
    "{\n",
    "    \"google\": [\"key1\", \"key2\", \"...\"],\n",
    "    \"openai\": [\"key1\", \"key2\", \"...\"],\n",
    "    \"huggingface\": [\"key1\", \"key2\", \"...\"]\n",
    "}\n",
    "```\n",
    "\n",
    "With this option, a load balancing of the keys will be performed and in case of quota limit errors, other keys will be tried.\n",
    "\n",
    "**NOTE**: Keep also a `api_keys_repo.json` as a backup. \n",
    "\n",
    "Why: In case of concurrent processes, they will fetch keys from `api_keys.json` and remove keys for load balancing purposes. The code has signal handlers to return them to the files, but something can go wrong and prevent that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Call LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`call_llm` is the main function of interest. It receives: \"messages\" to send to the model and \"generation config\" to specifiy: (i) the model's behavior (ii) the inference proviers/engines.\n",
    "\n",
    "Below a series of examples on how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms.llm_utils import call_llm, get_gen_config_fields, visualize_prompt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call to OpenAI, Google Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is one example of how the inputs can be provided for inference call. \n",
    "\n",
    "There are many variations possible. This file will include more in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[\n",
    "        {\"role\": \"system\", \"text\": \"You are an intelligent and helpful assistant.\"}, # dict with a role and text\n",
    "        {\"role\": \"user\", \"name\": \"example_user\", \"inputs\": [\"Example input 1\",\"./examples/cat.png\"]}, # a dict with user role, name and inputs\n",
    "        {\"role\": \"assistant\", \"name\": \"example_assistant\", \"contents\": [\"Example input 2\",\"./examples/dog.png\"]}, # a dict with assistant role, name and inputs\n",
    "        \"Describe **all** the below items.\", # raw string\n",
    "        [\"Item (1):\", \"./examples/cat.png\"], # A list with a prefix text and a file to an image (both are sent in the same message)\n",
    "        [\"Item (2):\", Image.open(\"./examples/dog.png\")], # A list with a prefix text and a PIL image (both are sent in the same message)\n",
    "        [\"Item (3):\", \"Once upon a time, there was a princess who lived in a castle.\"], # A list with only a text input\n",
    "        \"Provide your response as follows: <Title for Item 1> <Description for Item 1> <Title for Item 2> <Description for Item 2> <Title for Item 3> <Description for Item 3>\"\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTEs:** \n",
    " - Each of the entries in `inputs` is a `Message`. If that sounds confusing / ambiguous, check the file `llms.types.py` or continue reading.\n",
    " - In short: LLMs receive a series of `Message` objects, where a `Message` contains multiple raw inpus (such as images, text and video). The full prompt is a list of those messages.\n",
    " - We did not define a role for some entries (e.g.: `Item(1): ...`). These will default to `user` role. This behavior can be changed by providing the input as in the dictionary example or by creating a list of `Message` objects directly (see section (4))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize how the prompt will look like by using the `visualize_prompt` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will save an .html file with the prompt as it will be sent to the model.\n",
    "visualize_prompt(messages=inputs, output_path=\"./examples/vis.html\") \n",
    "\n",
    "# # To visualize in the jupyter notebook, run:\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "# # Read the HTML content from the file\n",
    "# with open(\"llms/examples/vis.html\", \"r\") as file:\n",
    "#     html_content = file.read()\n",
    "\n",
    "# # Display the content inline in the notebook\n",
    "# display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a minimum set of generation configs and call Gemini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gen_args = {\n",
    "    \"model\": \"gemini-2.0-flash-001\",\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"num_generations\": 1,\n",
    "}\n",
    "\n",
    "# If these are provided, HTML log of the conversation and CSV logs of the usage will be saved in the given directories\n",
    "conversation_dir = \"./examples/conversation\" # HTML and txt logs of the conversation saved here\n",
    "usage_dir = \"./examples/usage\" # CSV logs of the usage saved here\n",
    "call_id = \"test_call\" # If provided, the logs will be named like \"./examples/conversation/test_call.html\", \"./examples/usage/test_call.csv\", etc\n",
    "\n",
    "response, model_generations = call_llm(gen_args, inputs, conversation_dir=conversation_dir, usage_dir=usage_dir, call_id=call_id)    \n",
    "\n",
    "#NOTE: we did not specify a `role` for some of the inputs. \n",
    "# In this case, the `role` on any message not specified will be set to `user`.\n",
    "\n",
    "# You can specify the role in the dictionary way as above or by creating list `Message` objects as explained in (4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this call, we have\n",
    "- A list of `response` objects; these are dictionaries with data about the API request\n",
    "- A list of `model_generations`; these are `Message` objects containing the model's raw outputs (text, images, etc).\n",
    "- `html` and `txt` logs of the conversation round in the `llms/examples/conversation` directory\n",
    "- `csv` files with token usage information in the `llms/examples/usage` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE1**: For more of the `gen_args` available:\n",
    "- Please check `llms.generation_config.py` for more details on each one. Examples here will illustrate the main ones\n",
    "- The command below lists all possible parameters to control model behavior setting up providers/engines, but output is not pretty and some parameters are provider/engine specific. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gen_config_fields()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE 2:** Output format and accessing raw inputs\n",
    "\n",
    "Print below to see how the output is returned.\n",
    "\n",
    "The `Message` object is a unified format for both **inputs** and **outputs** of a user-model conversation. More details of it in `llms.types`, but in summary:\n",
    "- A single message contains: (i) a `role` that identifies the entity sending the information; (ii) data ('text', 'images', etc) sent by the entity\n",
    "- A conversation is a list of `Message` items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to see how the returned objects look like\n",
    "print(model_generations)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below some methods to access the `Message` raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all text content within a message\n",
    "print(model_generations[0].text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all images within a message (there is none in this case because this model outputs only text)\n",
    "model_generations[0].images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list with interleaved text, image, video, etc.\n",
    "model_generations[0].raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dict with format similar to OpenAI chat completion format\n",
    "model_generations[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Continuing the conversation + calling a new model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we want to send another query with the previous inputs + the model response + a new request.\n",
    "\n",
    "To make things more interesting, lets send this to **GPT4o** now.\n",
    "\n",
    "Below we construct this new input using the previous list of `inputs` and showing some new ways of providing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the new inputs\n",
    "new_inputs = inputs + [\n",
    "    model_generations[0], # The Message object can be sent directly as input too; notice it contains the ROLE of the entity.\n",
    "    {\"role\": \"user\", \"text\": \"Please give an opinion of the above conversation. How do you evaluate the assistant's performance?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the prompt before sending for a sanity check by using the `visualize_prompt` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"llms/examples/vis.html\"\n",
    "visualize_prompt(new_inputs, output_path)\n",
    "# This commands save an `.html` file with the messsages as they will be received by the model. \n",
    "# Open it in a browser for visualization and to sanity check if the order of messages, roles, entitiy names, etc is correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making sure the prompt is correct, we can send it to GPT4o. \n",
    "- For that, we only need to change the `model` parameter in the previous generation arguments.\n",
    "- There is no need to adjust parameter names or values to abide to the new provider. \n",
    "- Same thing for the prompt formats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only change the model name in the generation config.\n",
    "gen_args[\"model\"] = \"gpt-4o-2024-08-06\"\n",
    "# Let's also ask for 2 generations\n",
    "gen_args[\"num_generations\"] = 2\n",
    "\n",
    "# You can add a `call_id` to save the conversation and usage logs with a specific name.\n",
    "# in this case: \"./examples/conversation/gpt4o_call.html\", \"./examples/conversation/gpt4o_call.txt\", \"./examples/usage/gpt4o_call.csv\", etc\n",
    "response, model_generations = call_llm(gen_args, new_inputs, conversation_dir=conversation_dir, usage_dir=usage_dir, call_id=\"gpt4o_call\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the models answers\n",
    "for msg in model_generations:\n",
    "    print(msg.text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: add more examples:\n",
    "- OpenAI's `response` API\n",
    "- Batch generation\n",
    "- Other variations for input formats and dictionary keys\n",
    "- Multimodal outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same process to call models from HuggingFace's, except that:\n",
    "\n",
    "- (i) We need to specify some more arguments such as \"engine\" to deploy the model\n",
    "    - Supported engines are: `automodel`, `server`, `vllm` and `openai`. Details below.\n",
    "- (ii) There is a higher likelihood of bugs; many models in HuggingFace have model-specific quirks and it is impossible to foresee all them.\n",
    "    - The code will do the best effort to process the inputs and generate the outputs. But for instance, `Qwen-2.5-VL` was not supported by the `Automodel` class so there is a specific handling of model loading and generation that is hard to automate. \n",
    "    - Moreover, some models have specific prompts that are not always covered by the `apply_chat_template`. \n",
    "    - etc\n",
    "- We can also specify other args like: which resources to use (e.g.: CPU, GPU, etc); if quantize or not; etc. See `llms.generation_config.py` for all HF-specifc args."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below examples make an inference call to `Qwen-2.5-VL-3B` using the three engines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the same `inputs` as above, but with other examples of ways to send each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[\n",
    "        {\"role\": \"system\", \"text\": \"You are an intelligent and helpful assistant.\"}, \n",
    "        \"Describe **all** the below items.\",\n",
    "        {\"role\": \"user\", \"text\": \"Item (1):\", \"image\": \"llms/examples/cat.png\"}, # Another way to send an input\n",
    "        {\"role\": \"user\", \"contents\":[{\"type\": \"text\", \"text\": \"Item (2):\"}, {\"type\": \"image\", \"image\": \"llms/examples/dog.png\"}]}, #OpenAI chat completion format\n",
    "        \"Item (3): Once upon a time, there was a princess who lived in a castle.\"\n",
    "        \"Provide your response as follows: <Title for Item 1> <Description for Item 1> <Title for Item 2> <Description for Item 2> <Title for Item 3> <Description for Item 3>\"\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize the prompt\n",
    "\n",
    "visualize_prompt(inputs, \"llms/examples/vis_hf.html\")\n",
    "# Read the HTML content from the file\n",
    "with open(\"llms/examples/vis.html\", \"r\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Display the content inline in the notebook\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging Face - Automodel Engine\n",
    "\n",
    "\n",
    "This mode is the same as the vanilla usage of hugging face; the model is available only to the current process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_args = {\n",
    "    \"model\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    \"engine\": \"automodel\",\n",
    "    \"num_generations\": 1,\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "\n",
    "    # \"flash_attn\": True,    # Code will automatically try to use if available\n",
    "    # \"torch_dtype\": \"auto\", # Code will automatically choose based on model info\n",
    "    # \"device\": \"auto\",      # Code will determine based on machine and other params. Typically sets to 'auto'\n",
    "    # \"quant_bits\": \"int8\",  # Quantizes to int8 if supported by model. \"int4\" also supported.\n",
    "}\n",
    "conversation_dir = \"llms/examples/conversation\"\n",
    "usage_dir = \"llms/examples/usage\"\n",
    "responses, model_generations = call_llm(gen_args, inputs, conversation_dir=conversation_dir, usage_dir=usage_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "- By default: \n",
    "    - `device_map=auto`. Set `device:<device>` to override. #TODO: allow dict with `device_map`;\n",
    "    - Use `flash_attn` if it is available. To disable, set `flash_attn:False`\n",
    "    - Set `dtype` based on the model information and if not found, it set to `auto`. Set `dtype` to override.\n",
    "- Behind the scenes, the prompts are converted to an OpenAI chat completions format that HF uses. Check them via `responses[idx][\"prompt\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generations[0].text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging Face - Local Server Engine\n",
    "\n",
    "The `server` engine makes model available at an `endpoint`, so multiple processes can send inference requests without using multiple GPUs.\n",
    "\n",
    "There are two ways to deploy in this mode:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1: (Recommeded) Host the model first, then send inference calls with `call_llm`**\n",
    "\n",
    "\n",
    "1. Run:\n",
    "\n",
    " ```bash\n",
    " python -m llms.providers.hugging_face.host_model_hf \"Qwen/Qwen2.5-VL-3B-Instruct\" --host <host> --port <port>\n",
    " ```\n",
    "\n",
    "2. Add `engine:server` and `<host>:<port>` in `gen_args`\n",
    "\n",
    "**NOTE**: If hosting in `machineA` and accessing model via `machineB`: execute step 1 in machineA; to `call_llm` from `machineB`, set `host` to the IP of machineA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Option 2:  Directly call `call_llm` with `engine:server` and `localhost:<port>` in `gen_args`.**\n",
    "- This will automatically host the model if possible, using the same script `llms.providers.hugging_face.host_model_hf`\n",
    "- It is less recommended as:\n",
    "    - The process hosting the model will die if the first process that calls `call_llm` ends\n",
    "    - For new models, weights will be downloaded; the code wait for the server to start, but it can take a while and you may get false positives saying server was unable to start.\n",
    "    - All kinds of problems if there are concurrent processes that need to wait for the same server to start\n",
    "- Use this mostly to prototype using single process. Do not use for concurrent execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`call_llm` example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we ran:\n",
    "# python -m llms.providers.hugging_face.host_model_hf \"Qwen/Qwen2.5-VL-3B-Instruct\" --host localhost --port 8000\n",
    "\n",
    "# Then we can send inference to this server by adding these args in `gen_args`:\n",
    "gen_args = {\n",
    "    \"model\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    \"num_generations\": 1,\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "    \"engine\": \"server\",  # <--------- CHANGED `automodel` to `server`\n",
    "    \"endpoint\": \"localhost:8000\"  # <--------- ADDED\n",
    "}\n",
    "\n",
    "# No need for any change in the inputs.\n",
    "\n",
    "response, model_generations = call_llm(gen_args, inputs, conversation_dir=conversation_dir, usage_dir=usage_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging Face - VLLM Engine\n",
    "\n",
    "The `vllm` makes model available to receive requests at `endpoint`, so multiple processes can send inference requests without using multiple GPUs.\n",
    "\n",
    "NOTES:\n",
    "- The idea is the same as `server`, but in this case the server is handled by `vllm`\n",
    "- `vllm` has non-trivial optimization to handle concurrent calls. May be a better option in cases of high demand for the server.\n",
    "- Issue: `vllm` tends to consume a lot of GPU memory to realize its optimizations. \n",
    "    - You may run out of memory even for models that are typically possible to load with vanilla automodel.\n",
    "    - In these cases, try to increase `--gpu-mem` (between 0 and 1), do not pass `--enforce-eager` (set to false), and reduce `--max-model-len`.\n",
    "\n",
    "\n",
    "There are two ways to deploy in this mode:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1: Host the model first, then send inference calls with `call_llm`**\n",
    "\n",
    "1. Run \n",
    "\n",
    "```bash\n",
    "python -m llms.providers.hugging_face.host_model_vllm <model_id> --host <host> --port <port> --num-gpus <num_gpus> --max-model-len <max_model_len>` \n",
    "# (check all params using -h)\n",
    "```\n",
    "\n",
    "2. Add `engine:vllm` and `<host>:<port>` in `gen_args`\n",
    "\n",
    "**NOTE**: If hosting in `machineA` and accessing model via `machineB`: execute step 1 in machineA; to `call_llm` from `machineB`, set `host` to the IP of machineA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Directly call `call_llm` with `engine:vllm` and `<host>:<port>` in `gen_args`.**\n",
    "- All the warnings from the `server` case apply here too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`call_llm` example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obs.: needs to have an API key for the corresponding provider.\n",
    "\n",
    "# Then we can send inference to this server by adding these args in `gen_args`:\n",
    "gen_kwargs = {\n",
    "    \"model\": \"qwen/qwen2.5-vl-72b-instruct:free\",\n",
    "    \"engine\": \"openai\",\n",
    "    \"metadata\": {\n",
    "        \"base_url\": \"https://openrouter.ai/api/v1\",\n",
    "        \"provider\": \"openrouter\",\n",
    "    },\n",
    "    \"num_generations\": 1,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_tokens\": 256,\n",
    "}\n",
    "\n",
    "# No need for any change in the inputs.\n",
    "response, model_generations = call_llm(gen_args, inputs, conversation_dir=conversation_dir, usage_dir=usage_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face - thirdy-party providers that use OpenAI client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Prompting and get_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms.prompt_utils import get_messages, get_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions `get_messages` and `get_message` gives more fine-grained control to send the prompts. \n",
    "- Obs.: Anything can also be done via the flexible list of inputs used in (3).\n",
    "\n",
    "The function `get_message` creates a single `Message` object given:\n",
    "- `inputs`: list of raw data in flexible format (same way as given to `call_llm` as explained in (3))\n",
    "- `role`: of the entity responsible for the message\n",
    "- `name` of the entity responsible for the message\n",
    "- `img_detail`: for providers that support, defines how much details to apply to the image\n",
    "\n",
    "`get_messages` Is the same thing, but gives you a list of Message objects instead. It also allows:\n",
    "- to give the `sys_prompt` via an argument as well.\n",
    "- concatenate consecutive texts into one `Message` by setting `concatenate_text=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the same `inputs` as before. We can create a list of Message objects from it as below. This is exactly what `call_llm` does behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[\n",
    "        {\"role\": \"system\", \"text\": \"You are an intelligent and helpful assistant.\"}, \n",
    "        \"Describe **all** the below items.\",\n",
    "        {\"role\": \"user\", \"text\": \"Item (1):\", \"image\": \"llms/examples/cat.png\"}, # Another way to send an input\n",
    "        {\"role\": \"user\", \"contents\":[{\"type\": \"text\", \"text\": \"Item (2):\"}, {\"type\": \"image\", \"image\": \"llms/examples/dog.png\"}]}, #OpenAI chat completion format\n",
    "        \"Item (3): Once upon a time, there was a princess who lived in a castle.\"\n",
    "        \"Provide your response as follows: <Title for Item 1> <Description for Item 1> <Title for Item 2> <Description for Item 2> <Title for Item 3> <Description for Item 3>\"\n",
    "  ]\n",
    "\n",
    "# Create a list of Message objects from the inputs\n",
    "messages = get_messages(inputs)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a message object with higher image detail; note we can also give a `name` to the user (some providers support it)\n",
    "msg_ex_user = get_message([\"Item (1):\", \"llms/examples/cat.png\"], role=\"user\", name=\"example_user\", img_detail=\"high\")\n",
    "\n",
    "# Create an ASSISTANT message; note we can also give a `name` to the assistant (some providers support it)\n",
    "msg_ex_assistant = get_message([\"This is a cat\"], role=\"assistant\", name=\"example_assistant\")\n",
    "\n",
    "# Create a SYSTEM message\n",
    "msg_system = get_message(\"You are an intelligent and helpful assistant.\", role=\"system\")\n",
    "\n",
    "\n",
    "# get a full prompt to send to the model\n",
    "get_messages(\n",
    "    [\n",
    "        msg_system,\n",
    "        msg_ex_user,\n",
    "        msg_ex_assistant,\n",
    "        {\"role\": \"user\", \"contents\":[{\"type\": \"text\", \"text\": \"Item (2):\"}, {\"type\": \"image\", \"image\": \"llms/examples/dog.png\"}]}, #OpenAI chat completion format\n",
    "        \"Item (3): Once upon a time, there was a princess who lived in a castle.\"\n",
    "        \"Please describe the new items in the conversation.\"\n",
    "    ],\n",
    "    concatenate_text=True, # Concatenate consecutive texts into one `Message`. Note the last two are all in the same message.\n",
    "    role=\"user\", # This role is applied to all messages without a role. (e.g.: last two)\n",
    "    name=\"user\", # This name is applied to all messages without a name. (e.g.: last two)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Batch Call LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically sends a batch of messages to MLLM and receives a batch of responses.\n",
    "- For API providers, parallel calls are performed behind the scenes\n",
    "- For HF, uses the usual inference with tensors. \n",
    "- Just give a long list of prompts and set `max_batch_size` to control how many messages are sent to the model per time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms.llm_utils import batch_call_llm\n",
    "\n",
    "# TODO: add examples / explanation\n",
    "\n",
    "gen_args = {\n",
    "    \"model\": \"gpt-4o-2024-08-06\",\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "msgs1= [\n",
    "    {\"role\": \"system\", \"text\": \"You are an intelligent and helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"text\": \"Item (1):\", \"image\": \"llms/examples/cat.png\"},\n",
    "    {\"role\": \"user\", \"text\": \"Item (2):\", \"image\": \"llms/examples/dog.png\"},\n",
    "    {\"role\": \"user\", \"text\": \"Item (3):\", \"image\": \"llms/examples/cat.png\"},\n",
    "    {\"role\": \"user\", \"text\": \"Item (4):\", \"image\": \"llms/examples/dog.png\"},\n",
    "    {\"role\": \"user\", \"text\": \"Item (5):\", \"image\": \"llms/examples/cat.png\"},\n",
    "]\n",
    "\n",
    "msgs2= [\n",
    "    {\"role\": \"system\", \"text\": \"You are an intelligent and helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"text\": \"Item (1):\", \"image\": \"llms/examples/cat.png\"},\n",
    "    {\"role\": \"user\", \"text\": \"Item (2):\", \"image\": \"llms/examples/dog.png\"},\n",
    "]\n",
    "conversation_dirs = [\"./conversation_dir1\", \"./conversation_dir2\"] \n",
    "usage_dirs = [\"./usage_dir1\", \"./usage_dir2\"]\n",
    "call_ids = [\"call1\", \"call2\"]   \n",
    "batch_call_llm(gen_kwargs=gen_args, \n",
    "               messages=[msgs1, msgs2], \n",
    "               max_batch_size=10, # Max number of messages to send in each batch\n",
    "               conversation_dirs=conversation_dirs, \n",
    "               usage_dirs=usage_dirs,\n",
    "               call_ids=call_ids\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
