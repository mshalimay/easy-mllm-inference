{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: /home/mashalimay/webarena/modular_agent\n"
     ]
    }
   ],
   "source": [
    "# Just some tricks to look like we are running comands from the root directory\n",
    "\n",
    "# Get the current working directory\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to system path (2 levels up from current directory)\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, \"../..\"))\n",
    "sys.path.append(parent_dir)\n",
    "import os\n",
    "\n",
    "# Change the working directory to the project root (assuming the root is 2 levels up)\n",
    "os.chdir(os.path.abspath(os.path.join(os.getcwd(), \"../..\")))\n",
    "print(\"Working directory set to:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Enable the autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Other Imports\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Overview\n",
    "\n",
    "The idea of th `llms` package is to make it very simple to send inference to LLMs. \n",
    "In summary: it allows inference calls to any of the supported providers and engines with something as simple as: `messages=list[images, text, video, function_call, ...]` and `generation_configs = {k:v, k:v,..}`. \n",
    "\n",
    "\n",
    "And behind the scenes, the package handles:\n",
    "- Formatting of prompts for providers and models.\n",
    "- Setting up clients, inference engines for HuggingFace\n",
    "- Routing the inference call to the appropriate provider and engine\n",
    "- Load balancing API keys\n",
    "- Retry with exponential backoff with customized error logic\n",
    "- Logging of inputs and outputs, including: HTML visualization of the prompt for debugging; token counts; conversation logs\n",
    "- Return of multimodal outputs in a unified format independent of the provider, model, or provider mode.\n",
    "- Type validation and handling of multiple media types\n",
    "- Etc, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Setting API keys\n",
    "\n",
    "Two alternatives:\n",
    "\n",
    "#### 1) Set the API keys in the environment variables.\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"<key >\"\n",
    "export GOOGLE_API_KEY=\"<key>\"\n",
    "export HF_TOKEN=\"<key>\n",
    "```\n",
    "In this option, **no load balancing** of the keys is performed in case of multiple calls.\n",
    "\n",
    "#### 2)(Recommended) Create a `api_keys.json` file  as follows:\n",
    "\n",
    "api_keys.json\n",
    "```json\n",
    "{\n",
    "    \"google\": [\"key1\", \"key2\", \"...\"],\n",
    "    \"openai\": [\"key1\", \"key2\", \"...\"],\n",
    "    \"huggingface\": [\"key1\", \"key2\", \"...\"]\n",
    "}\n",
    "```\n",
    "\n",
    "With this option, a load balancing of the keys will be performed and in case of quota limit errors, other keys will be tried.\n",
    "\n",
    "**NOTE**: Keep also a `api_keys_repo.json` as a backup. Why: in case of concurrent processes, they will fetch keys from `api_keys.json` and remove keys for load balancing purposes and if something unexpected happens, they may not be returned to the file \n",
    "\n",
    "(though the code have fallbacks to return the keys if the processes are killed, many things can go wrong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Call LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`call_llm` is the main function of interest. It receives: \"messages\" to send to the model and \"generation config\" to specifiy: (i) the model's behavior (ii) the inference proviers/engines.\n",
    "\n",
    "Below a series of examples on how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mashalimay/miniconda3/envs/vwebarena/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llms.llm_utils import call_llm, get_gen_config_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below lists of all possible parameters to control model behavior setting up providers/engines. \n",
    "\n",
    "Obs.: The output is not pretty yet and some parameters are provider/engine specific. Please check `llms.generation_config.py` for more details on each one. Examples that follows illustrate the main ones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gen_config_fields()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call to OpenAI, Google Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is one example of how the inputs can be provided for inference call. \n",
    "\n",
    "There are many variations possible. This file will include more in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[\n",
    "        {\"role\": \"system\", \"text\": \"You are an intelligent and helpful assistant.\"}, # dict with a role and text\n",
    "        \"Describe **all** the below items.\", # raw string\n",
    "        [\"Item (1):\", \"llms/examples/cat.png\"], # A list with a prefix text and a file to an image (both are sent in the same message)\n",
    "        [\"Item (2):\", Image.open(\"llms/examples/dog.png\")], # A list with a prefix text and a PIL image (both are sent in the same message)\n",
    "        [\"Item (3):\", \"Once upon a time, there was a princess who lived in a castle.\"], # A list with only a text input\n",
    "        \"Provide your response as follows: <Title for Item 1> <Description for Item 1> <Title for Item 2> <Description for Item 2> <Title for Item 3> <Description for Item 3>\"\n",
    "  ]\n",
    "\n",
    "# **NOTE:** Each of the `inputs` entries is a `message`. If that sounds confusing / ambiguous, check the file `llms.types.py` or continue reading.\n",
    "# In short: LLMs receive a series of `message` objects, where \n",
    "# a `message` object contains multiple raw inpus (such as images, text and video). \n",
    "# The full prompt to the llm is a list of those messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a minimum set of generation configs and call Gemini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CALLING MODEL: `gemini-2.0-flash-001`: generating 1 outputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new executor for llms.providers.google.google_utils.sync_api_call\n"
     ]
    }
   ],
   "source": [
    "gen_args = {\n",
    "    \"model\": \"gemini-2.0-flash-001\",\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"num_generations\": 1,\n",
    "}\n",
    "conversation_dir = \"llms/examples/conversation\"\n",
    "usage_dir = \"llms/examples/usage\"\n",
    "response, model_generations = call_llm(gen_args, inputs, conversation_dir=conversation_dir, usage_dir=usage_dir)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this call, we have\n",
    "- A list of `response` objects; these are dictionaries with data about the API request\n",
    "- A list of `model_generations`; these are `Message` objects containing the model's raw outputs (text, images, etc).\n",
    "- `html` and `txt` logs of the conversation round in the `llms/examples/conversation` directory\n",
    "- `csv` files with token usage information in the `llms/examples/usage` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Message(role='assistant', contents=[ContentItem(type='text', data='**Item 1: Tabby Cat**\\nThe image shows a tabby cat sitting upright. It has brown and black stripes, green eyes, and a pink nose. The cat is sitting on a ledge or wall, and there are bare branches in the background.\\n\\n**Item 2: Golden Retriever Puppy**\\nThe image shows a golden retriever puppy sitting in a grassy field. The puppy has a light golden coat and an open mouth, as if panting or smiling. There are orange flowers scattered in the background.\\n\\n**Item 3: Story Starter**\\nThe text is the beginning of a fairy tale: \"Once upon a time, there was a princess who lived in a castle.\" This is a classic opening line for a children\\'s story, setting the scene for a narrative about royalty and potentially adventure.\\n', meta_data={}, id=None)], name='', meta_data={})]\n",
      "[{'candidates': [{'content': {'parts': [{'video_metadata': None, 'thought': None, 'code_execution_result': None, 'executable_code': None, 'file_data': None, 'function_call': None, 'function_response': None, 'inline_data': None, 'text': '**Item 1: Tabby Cat**\\nThe image shows a tabby cat sitting upright. It has brown and black stripes, green eyes, and a pink nose. The cat is sitting on a ledge or wall, and there are bare branches in the background.\\n\\n**Item 2: Golden Retriever Puppy**\\nThe image shows a golden retriever puppy sitting in a grassy field. The puppy has a light golden coat and an open mouth, as if panting or smiling. There are orange flowers scattered in the background.\\n\\n**Item 3: Story Starter**\\nThe text is the beginning of a fairy tale: \"Once upon a time, there was a princess who lived in a castle.\" This is a classic opening line for a children\\'s story, setting the scene for a narrative about royalty and potentially adventure.\\n'}], 'role': 'model'}, 'citation_metadata': None, 'finish_message': None, 'token_count': None, 'avg_logprobs': -0.27875192481351185, 'finish_reason': 'STOP', 'grounding_metadata': None, 'index': None, 'logprobs_result': None, 'safety_ratings': [{'blocked': None, 'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'probability_score': None, 'severity': None, 'severity_score': None}, {'blocked': None, 'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'probability_score': None, 'severity': None, 'severity_score': None}, {'blocked': None, 'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'probability_score': None, 'severity': None, 'severity_score': None}, {'blocked': None, 'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'probability_score': None, 'severity': None, 'severity_score': None}]}], 'create_time': None, 'response_id': None, 'model_version': 'gemini-2.0-flash-001', 'prompt_feedback': None, 'usage_metadata': {'cached_content_token_count': None, 'candidates_token_count': 166, 'prompt_token_count': 607, 'total_token_count': 773}, 'automatic_function_calling_history': [], 'parsed': None}]\n"
     ]
    }
   ],
   "source": [
    "# Just to see how the returned objects look like\n",
    "print(model_generations)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Message` object is a unified format for both **inputs** and **outputs** of a user-model conversation. More details of it in `llms.types`, but in summary:\n",
    "- A single message contains: (i) a `role` that identifies the entity sending the information; (ii) data ('text', 'images', etc) sent by the entity\n",
    "- A conversation is a list of `Message` items.\n",
    "\n",
    "**NOTE** we didn't specify the role of many of the entries in the `inputs` above. In this case, they will be assumed to be role `user`. There are a couple ways to change this behavior which we'll see below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below some methods to access the `Message`s raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Item 1: Tabby Cat**\\nThe image shows a tabby cat sitting upright. It has brown and black stripes, green eyes, and a pink nose. The cat is sitting on a ledge or wall, and there are bare branches in the background.\\n\\n**Item 2: Golden Retriever Puppy**\\nThe image shows a golden retriever puppy sitting in a grassy field. The puppy has a light golden coat and an open mouth, as if panting or smiling. There are orange flowers scattered in the background.\\n\\n**Item 3: Story Starter**\\nThe text is the beginning of a fairy tale: \"Once upon a time, there was a princess who lived in a castle.\" This is a classic opening line for a children\\'s story, setting the scene for a narrative about royalty and potentially adventure.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all text content within a message\n",
    "model_generations[0].text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all images within a message (there is none in this case because this model outputs only text)\n",
    "model_generations[0].images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['**Item 1: Tabby Cat**\\nThe image shows a tabby cat sitting upright. It has brown and black stripes, green eyes, and a pink nose. The cat is sitting on a ledge or wall, and there are bare branches in the background.\\n\\n**Item 2: Golden Retriever Puppy**\\nThe image shows a golden retriever puppy sitting in a grassy field. The puppy has a light golden coat and an open mouth, as if panting or smiling. There are orange flowers scattered in the background.\\n\\n**Item 3: Story Starter**\\nThe text is the beginning of a fairy tale: \"Once upon a time, there was a princess who lived in a castle.\" This is a classic opening line for a children\\'s story, setting the scene for a narrative about royalty and potentially adventure.\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A list with interleaved text, image, video, etc.\n",
    "model_generations[0].raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contents': [{'type': 'text',\n",
       "   'data': '**Item 1: Tabby Cat**\\nThe image shows a tabby cat sitting upright. It has brown and black stripes, green eyes, and a pink nose. The cat is sitting on a ledge or wall, and there are bare branches in the background.\\n\\n**Item 2: Golden Retriever Puppy**\\nThe image shows a golden retriever puppy sitting in a grassy field. The puppy has a light golden coat and an open mouth, as if panting or smiling. There are orange flowers scattered in the background.\\n\\n**Item 3: Story Starter**\\nThe text is the beginning of a fairy tale: \"Once upon a time, there was a princess who lived in a castle.\" This is a classic opening line for a children\\'s story, setting the scene for a narrative about royalty and potentially adventure.\\n',\n",
       "   'meta_data': {},\n",
       "   'id': None}],\n",
       " 'role': 'assistant',\n",
       " 'name': '',\n",
       " 'meta_data': {}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A dict with format similar to OpenAI chat completion format\n",
    "model_generations[0].to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we want to send another query with the previous inputs + the model response + a new request.\n",
    "\n",
    "To make things more interesting, lets send this to **GPT4o** now.\n",
    "\n",
    "Below we construct this new input using the previous list of `inputs` and showing some new ways of providing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the new inputs\n",
    "new_inputs = inputs + [\n",
    "    model_generations[0], # The Message object can be sent directly as input too; notice it contains the ROLE of the entity!\n",
    "    {\"role\": \"user\", \"text\": \"Please give an opinion of the above conversation. How do you evaluate the assistant's performance?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the prompt before sending for a sanity check by using the `visualize_prompt` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms.llm_utils import visualize_prompt\n",
    "output_path = \"llms/examples/vis.html\"\n",
    "visualize_prompt(new_inputs, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This commands save an `.html` file with the messsages as they will be received by the model. \n",
    "\n",
    "Open it in a browser for visualization and to sanity check if the order of messages, roles, entitiy names, etc is correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to check how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Read the HTML content from the file\n",
    "with open(\"llms/examples/vis.html\", \"r\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Display the content inline in the notebook\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making sure the prompt is correct, we can send it to GPT4o. \n",
    "- For that, we only need to change the `model` parameter in the previous generation arguments.\n",
    "- There is no need to adjust parameter names or values to abide to the new provider. \n",
    "- Same thing for the prompt formats!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only change the model name in the generation config.\n",
    "gen_args[\"model\"] = \"gpt-4o-2024-08-06\"\n",
    "\n",
    "# You can add a `call_id` to save the conversation and usage logs with a specific name.\n",
    "response, model_generations = call_llm(gen_args, new_inputs, conversation_dir=conversation_dir, usage_dir=usage_dir, call_id=\"gpt4o_call\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The assistant's performance was accurate and concise. It provided clear descriptions of the images and text, adhering to the requested format. The descriptions for the cat and puppy were detailed, capturing key features and setting. The story starter was correctly identified as a classic fairy tale opening. Overall, the response was well-structured and informative.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_generations[0].text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: add more examples:\n",
    "- OpenAI's `response` API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same process to call models from HuggingFace's, except that:\n",
    "\n",
    "- (i) We need to specify some more arguments such as \"engine\" to deploy the model\n",
    "    - Supported engines are: `autmodel`, `server`, and `vllm`. Details below.\n",
    "- (ii) There is a higher likelihood of bugs; many models in HuggingFace have model-specific quirks and it is impossible to foresee all them.\n",
    "    - The code will do the best effort to process the inputs and generate the outputs. But for instance, `Qwen-2.5-VL` was not supported by the `Automodel` class so there is a specific handling of model loading and generation that is hard to automate. \n",
    "    - Moreover, some models have specific prompts that are not always covered by the `apply_chat_template`. \n",
    "    - etc\n",
    "- We can also specify other args like: which resources to use (e.g.: CPU, GPU, etc); if quantize or not; etc. See `llms.generation_config.py` for all HF-specifc args."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below examples make an inference call to `Qwen-2.5-VL-3B` using the three engines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the same `inputs` as above, but with other examples of ways to send each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[\n",
    "        {\"role\": \"system\", \"text\": \"You are an intelligent and helpful assistant.\"}, \n",
    "        \"Describe **all** the below items.\",\n",
    "        {\"role\": \"user\", \"text\": \"Item (1):\", \"image\": \"llms/examples/cat.png\"}, # Another way to send an input\n",
    "        {\"role\": \"user\", \"contents\":[{\"type\": \"text\", \"text\": \"Item (2):\"}, {\"type\": \"image\", \"image\": \"llms/examples/dog.png\"}]}, #OpenAI chat completion format\n",
    "        \"Item (3): Once upon a time, there was a princess who lived in a castle.\"\n",
    "        \"Provide your response as follows: <Title for Item 1> <Description for Item 1> <Title for Item 2> <Description for Item 2> <Title for Item 3> <Description for Item 3>\"\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize the prompt\n",
    "\n",
    "visualize_prompt(inputs, \"llms/examples/vis_hf.html\")\n",
    "# Read the HTML content from the file\n",
    "with open(\"llms/examples/vis.html\", \"r\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Display the content inline in the notebook\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging Face - Automodel Engine\n",
    "\n",
    "\n",
    "This mode is the same as the vanilla usage of hugging face; the model is available only to the current process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[/home/mashalimay/webarena/modular_agent/llms/providers/hugging_face/hf_utils.py] CALLING MODEL: `Qwen/Qwen2.5-VL-3B-Instruct` with engine `automodel`: generating 1 output(s)...\n"
     ]
    }
   ],
   "source": [
    "gen_args = {\n",
    "    \"model\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    \"engine\": \"automodel\",\n",
    "    \"num_generations\": 1,\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "}\n",
    "conversation_dir = \"llms/examples/conversation\"\n",
    "usage_dir = \"llms/examples/usage\"\n",
    "responses, model_generations = call_llm(gen_args, inputs, conversation_dir=conversation_dir, usage_dir=usage_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "- By default: \n",
    "    - `device_map=auto`. Set `device:<device>` to override. #TODO: allow dict with `device_map`;\n",
    "    - Use `flash_attn` if it is available. To disable, set `flash_attn:False`\n",
    "    - Set `dtype` based on the model information and if not found, it set to `auto`. Set `dtype` to override.\n",
    "- Behind the scenes, the prompts are converted to an OpenAI chat completions format that HF uses. Check them via `responses[idx][\"prompt\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<Title for Item 1>: Tabby Cat\\n\\n<Description for Item 1>: The image shows a tabby cat with a striped coat sitting on a ledge. The cat has a mix of dark and light fur patterns, with green eyes. It appears to be looking directly at the camera with a calm and alert expression.\\n\\n<Title for Item 2>: Golden Retriever\\n\\n<Description for Item 2>: The image features a golden retriever dog standing on a grassy field. The dog is looking up, possibly at something interesting in the sky or in the distance. The background is filled with orange flowers, creating a vibrant and colorful scene.\\n\\n<Title for Item 3>: Princess in a Castle\\n\\n<Description for Item 3>: This item is a story setting that describes a princess living in a castle. The princess is the central character in this narrative, and her life within the castle is the focus of the story. The castle provides a backdrop of grandeur and mystery, with its tall walls and intricate architecture. The princess's daily life, adventures, and relationships with other characters in the castle are the main elements of the story.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_generations[0].text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging Face - Local Server Engine\n",
    "\n",
    "The `server` engine makes model available at an `endpoint`, so multiple processes can send inference requests without using multiple GPUs.\n",
    "\n",
    "There are two ways to deploy in this mode:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1: (Recommeded) Host the model first, then send inference calls with `call_llm`**\n",
    "\n",
    "\n",
    "1. Run:\n",
    "\n",
    " ```bash\n",
    " python -m llms.providers.hugging_face.host_model_hf \"Qwen/Qwen2.5-VL-3B-Instruct\" --host <host> --port <port>\n",
    " ```\n",
    "\n",
    "2. Add `engine:server` and `<host>:<port>` in `gen_args`\n",
    "\n",
    "**NOTE**: If hosting in `machineA` and accessing model via `machineB`: execute step 1 in machineA; to `call_llm` from `machineB`, set `host` to the IP of machineA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Option 2:  Directly call `call_llm` with `engine:server` and `localhost:<port>` in `gen_args`.**\n",
    "- This will automatically host the model if possible, using the same script `llms.providers.hugging_face.host_model_hf`\n",
    "- It is less recommended as:\n",
    "    - The process hosting the model will die if the first process that calls `call_llm` ends\n",
    "    - For new models, weights will be downloaded; the code wait for the server to start, but it can take a while and you may get false positives saying server was unable to start.\n",
    "    - All kinds of problems if there are concurrent processes that need to wait for the same server to start\n",
    "- Use this mostly to prototype using single process. Do not use for concurrent execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`call_llm` example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we ran:\n",
    "# python -m llms.providers.hugging_face.host_model_hf \"Qwen/Qwen2.5-VL-3B-Instruct\" --host localhost --port 8000\n",
    "\n",
    "# Then we can send inference to this server by adding these args in `gen_args`:\n",
    "gen_args = {\n",
    "    \"model\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    \"num_generations\": 1,\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "    \"engine\": \"server\",  # <--------- CHANGED `automodel` to `server`\n",
    "    \"endpoint\": \"localhost:8000\"  # <--------- ADDED\n",
    "}\n",
    "\n",
    "# No need for any change in the inputs.\n",
    "\n",
    "response, model_generations = call_llm(gen_args, inputs, conversation_dir=conversation_dir, usage_dir=usage_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging Face - VLLM Engine\n",
    "\n",
    "The `vllm` makes model available to receive requests at `endpoint`, so multiple processes can send inference requests without using multiple GPUs.\n",
    "\n",
    "NOTES:\n",
    "- The idea is the same as `server`, but in this case the server is handled by `vllm`\n",
    "- `vllm` has non-trivial optimization to handle concurrent calls. May be a better option in cases of high demand for the server.\n",
    "- Issue: `vllm` tends to consume a lot of GPU memory to realize its optimizations. \n",
    "    - You may run out of memory even for models that are typically possible to load with vanilla automodel.\n",
    "    - In these cases, try to increase `--gpu-mem` (between 0 and 1), do not pass `--enforce-eager` (set to false), and reduce `--max-model-len`.\n",
    "\n",
    "\n",
    "There are two ways to deploy in this mode:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1: Host the model first, then send inference calls with `call_llm`**\n",
    "\n",
    "1. Run \n",
    "\n",
    "```bash\n",
    "python -m llms.providers.hugging_face.host_model_vllm <model_id> --host <host> --port <port> --num-gpus <num_gpus> --max-model-len <max_model_len>` \n",
    "# (check all params using -h)\n",
    "```\n",
    "\n",
    "2. Add `engine:vllm` and `<host>:<port>` in `gen_args`\n",
    "\n",
    "**NOTE**: If hosting in `machineA` and accessing model via `machineB`: execute step 1 in machineA; to `call_llm` from `machineB`, set `host` to the IP of machineA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Directly call `call_llm` with `engine:vllm` and `<host>:<port>` in `gen_args`.**\n",
    "- All the warnings from the `server` case apply here too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`call_llm` example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we ran:\n",
    "# python -m llms.providers.hugging_face.host_model_vllm \"Qwen/Qwen2.5-VL-3B-Instruct\" --host localhost --port 8000\n",
    "\n",
    "# Then we can send inference to this server by adding these args in `gen_args`:\n",
    "gen_args = {\n",
    "    \"model\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    \"num_generations\": 1,\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "    \"engine\": \"server\",  # <--------- CHANGED `automodel` to `server`\n",
    "    \"endpoint\": \"localhost:8000\"  # <--------- ADDED\n",
    "}\n",
    "\n",
    "# No need for any change in the inputs.\n",
    "\n",
    "response, model_generations = call_llm(gen_args, inputs, conversation_dir=conversation_dir, usage_dir=usage_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Prompting and get_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms.prompt_utils import get_messages, get_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions `get_messages` and `get_message` gives more fine-grained control to send the prompts. \n",
    "- Obs.: Anything can also be done via the flexible list of inputs as explained in (3).\n",
    "\n",
    "The function `get_message` creates a single `Message` object given:\n",
    "- `inputs`: list of raw data in flexible format (same way as given to `call_llm` as explained in (3))\n",
    "- `role`: of the entity responsible for the message\n",
    "- `name` of the entity responsible for the message\n",
    "- `img_detail`: for providers that support, defines how much details to apply to the image\n",
    "\n",
    "`get_messages` Is the same thing, but gives you a list of Message objects instead. It also allows:\n",
    "- to give the `sys_prompt` via an argument as well.\n",
    "- concatenate consecutive texts into one `Message` by setting `concatenate_text=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the same `inputs` as before. We can create a list of Message objects from it as below. This is exactly what `call_llm` does behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Message(role='system', contents=[ContentItem(type='text', data='You are an intelligent and helpful assistant.', meta_data={}, id=None)], name='', meta_data={}),\n",
       " Message(role='user', contents=[ContentItem(type='text', data='Describe **all** the below items.', meta_data={}, id=None)], name='', meta_data={}),\n",
       " Message(role='user', contents=[ContentItem(type='text', data='Item (1):', meta_data={}, id=None), ContentItem(type='image', data=<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=439x582 at 0x7FF2F964FD10>, meta_data={}, id=None)], name='', meta_data={}),\n",
       " Message(role='user', contents=[ContentItem(type='text', data='Item (2):', meta_data={}, id=None), ContentItem(type='image', data=<PIL.PngImagePlugin.PngImageFile image mode=RGB size=234x148 at 0x7FF2F964FFD0>, meta_data={}, id=None)], name='', meta_data={}),\n",
       " Message(role='user', contents=[ContentItem(type='text', data='Item (3): Once upon a time, there was a princess who lived in a castle.Provide your response as follows: <Title for Item 1> <Description for Item 1> <Title for Item 2> <Description for Item 2> <Title for Item 3> <Description for Item 3>', meta_data={}, id=None)], name='', meta_data={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=[\n",
    "        {\"role\": \"system\", \"text\": \"You are an intelligent and helpful assistant.\"}, \n",
    "        \"Describe **all** the below items.\",\n",
    "        {\"role\": \"user\", \"text\": \"Item (1):\", \"image\": \"llms/examples/cat.png\"}, # Another way to send an input\n",
    "        {\"role\": \"user\", \"contents\":[{\"type\": \"text\", \"text\": \"Item (2):\"}, {\"type\": \"image\", \"image\": \"llms/examples/dog.png\"}]}, #OpenAI chat completion format\n",
    "        \"Item (3): Once upon a time, there was a princess who lived in a castle.\"\n",
    "        \"Provide your response as follows: <Title for Item 1> <Description for Item 1> <Title for Item 2> <Description for Item 2> <Title for Item 3> <Description for Item 3>\"\n",
    "  ]\n",
    "\n",
    "# Create a list of Message objects from the inputs\n",
    "messages = get_messages(inputs)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Message(role='system', contents=[ContentItem(type='text', data='You are an intelligent and helpful assistant.', meta_data={}, id=None)], name='', meta_data={}),\n",
       " Message(role='user', contents=[ContentItem(type='text', data='Item (1):', meta_data={}, id=None), ContentItem(type='image', data=<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=439x582 at 0x7FF2E9E7C810>, meta_data={'img_detail': 'high'}, id=None)], name='example_user', meta_data={}),\n",
       " Message(role='assistant', contents=[ContentItem(type='text', data='This is a cat', meta_data={}, id=None)], name='example_assistant', meta_data={}),\n",
       " Message(role='user', contents=[ContentItem(type='text', data='Item (2):', meta_data={}, id=None), ContentItem(type='image', data=<PIL.PngImagePlugin.PngImageFile image mode=RGB size=234x148 at 0x7FF2E9E7D890>, meta_data={}, id=None)], name='', meta_data={}),\n",
       " Message(role='user', contents=[ContentItem(type='text', data='Item (3): Once upon a time, there was a princess who lived in a castle.Please describe the new items in the conversation.', meta_data={}, id=None)], name='user', meta_data={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a message object with higher image detail; note we can also give a `name` to the user (some providers support it)\n",
    "msg_ex_user = get_message([\"Item (1):\", \"llms/examples/cat.png\"], role=\"user\", name=\"example_user\", img_detail=\"high\")\n",
    "\n",
    "# Create an ASSISTANT message; note we can also give a `name` to the assistant (some providers support it)\n",
    "msg_ex_assistant = get_message([\"This is a cat\"], role=\"assistant\", name=\"example_assistant\")\n",
    "\n",
    "# Create a SYSTEM message\n",
    "msg_system = get_message(\"You are an intelligent and helpful assistant.\", role=\"system\")\n",
    "\n",
    "\n",
    "# get a full prompt to send to the model\n",
    "get_messages(\n",
    "    [\n",
    "        msg_system,\n",
    "        msg_ex_user,\n",
    "        msg_ex_assistant,\n",
    "        {\"role\": \"user\", \"contents\":[{\"type\": \"text\", \"text\": \"Item (2):\"}, {\"type\": \"image\", \"image\": \"llms/examples/dog.png\"}]}, #OpenAI chat completion format\n",
    "        \"Item (3): Once upon a time, there was a princess who lived in a castle.\"\n",
    "        \"Please describe the new items in the conversation.\"\n",
    "    ],\n",
    "    concatenate_text=True, # Concatenate consecutive texts into one `Message`. Note the last two are all in the same message.\n",
    "    role=\"user\", # This role is applied to all messages without a role. (e.g.: last two)\n",
    "    name=\"user\", # This name is applied to all messages without a name. (e.g.: last two)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vwebarena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
