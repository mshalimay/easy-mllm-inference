# This file contains default parameters for some MLLMs as per reported by the corresponding model creators.
# It is used for for:
# (i) abstract away some required parameters from calls when possible, such the `provider` to use
# (ii) loading default parameters when desired (see `get_model_attribute` in `llm_utils.py`)
# (iii) promptly checking some information about a model

general:
  hf_models_location: "~/.cache/huggingface/"
  
models:
#llava-next
  llava-next-1.6:
    model_path: "llava-hf/llava-v1.6-vicuna-13b-hf"
    quant: ""
    provider: "huggingface"
    

# Llama-2 7b
  llama-2/7b:
    model_path: "meta-llama/Llama-2-7b-hf"
    tokenizer_path: "meta-llama/Llama-2-7b-hf"
    quant: ""
    content_len: 4096
    provider: "huggingface"

  llama-2/7b-chat:
    model_path: "meta-llama/Llama-2-7b-chat-hf"
    tokenizer_path: "meta-llama/Llama-2-7b-chat-hf"
    quant: ""
    context_win: 4096
    provider: "huggingface"

  llama-2/7b-32k-instruct:
    model_path: "togethercomputer/Llama-2-7B-32K-Instruct"
    tokenizer_path: "togethercomputer/Llama-2-7B-32K-Instruct"
    quant: ""
    context_win: 4096
    provider: "huggingface"
  
# Llama-2 7b quantized
  llama-2/7b-instruct-gptq:
    model_path: "TheBloke/Llama-2-7B-32K-Instruct-GPTQ"
    tokenizer_path: "TheBloke/Llama-2-7B-32K-Instruct-GPTQ"
    quant: "gptq"
    context_win: 4096
    provider: "huggingface"
  
  llama-2/7b-chat-gptq:
    model_path: "TheBloke/Llama-2-7b-Chat-GPTQ"
    tokenizer_path: "TheBloke/Llama-2-7b-Chat-GPTQ"
    quant: "gptq"
    context_win: 4096
    provider: "huggingface"

# Llama-2 13b quantized
  llama-2/13b-gptq:
    model_path: "TheBloke/LLaMA2-13B-Tiefighter-GPTQ"
    tokenizer_path: "TheBloke/Upstage-Llama-2-70B-instruct-v2-AWQ"
    context_win: 4096
    quant: "gptq"
    provider: "huggingface"

# Llama-3 8b 
  llama-3/8b:
    model_path: "meta-llama/Meta-Llama-3-8B"
    tokenizer_path: "meta-llama/Meta-Llama-3-8B"
    quant: ""
    context_win: 8192
    provider: "huggingface"
    dytpe: 'bfloat16'
    multimodal: 'false'
    memory_fp16: '16GB'
  
  llama-3/8b-instruct: 
  # base gen config: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/blob/main/generation_config.json
    model_path: "meta-llama/Meta-Llama-3-8B-Instruct"
    tokenizer_path: "meta-llama/Meta-Llama-3-8B-Instruct"
    quant: ""
    context_win: 8192
    provider: "huggingface"
    dytpe: 'bfloat16'
    multimodal: 'false'
    memory_fp16: '16GB'

  blip2:
    model_path: 'Salesforce/blip2-flan-t5-xl'
    quant: ""
  
  qwen_2_5_vl_3b:
    model_path: 'Qwen/Qwen2.5-VL-3B-Instruct'

# Google Models
  gemini-1.5-flash-001:
    model_path: 'models/gemini-1.5-flash-001'
    provider: 'google'
    input_token_limit: 1000000
    output_token_limit: 8192
    temperature: 1.0
    max_temperature: 2.0
    top_p: 0.95
    top_k: 40
    # See examples/gemini_api.py to see how to get this information

  gemini-1.5-flash-002:
    model_path: 'models/gemini-1.5-flash-002'
    provider: 'google'
    input_token_limit: 1000000
    output_token_limit: 8192
    temperature: 1.0
    max_temperature: 2.0
    top_p: 0.95
    top_k: 40

  gemini-1.5-pro-001:
    model_path: 'models/gemini-1.5-pro-001'
    provider: 'google'
    input_token_limit: 2000000
    output_token_limit: 8192
    temperature: 1.0
    max_temperature: 2.0
    top_p: 0.95
    top_k: 40
  
  gemini-1.5-pro-002:
    model_path: 'models/gemini-1.5-pro-002'
    provider: 'google'
    input_token_limit: 2000000
    output_token_limit: 8192
    temperature: 1.0
    max_temperature: 2.0
    top_p: 0.95
    top_k: 40

  gemini-2.0-flash-exp:
    model_path: 'models/gemini-2.0-flash-exp'
    provider: 'google'

  gemini-2.0-flash-001:
    model_path: 'models/gemini-2.0-flash-001'
    provider: 'google'
    temperature: 1.0,
    max_temperature: 2.0,
    top_p: 0.95,
    top_k: 40

  gemini-2.0-pro-exp:
    model_path: 'models/gemini-2.0-pro-exp'
    provider: 'google'
    temperature: 1.0,
    max_temperature: 2.0,
    top_p: 0.95,
    top_k: 40

  gemini-2.0-flash-exp-image-generation:
    model_path: 'models/gemini-2.0-flash-exp-image-generation'
    provider: 'google'

  gemini-2.0-flash-thinking-exp-1219:
    model_path: 'models/gemini-2.0-flash-thinking-exp-1219'
    provider: 'google'

# OpenAI models
  gpt-3.5-turbo-0125:
    model_path: 'gpt-3.5-turbo-0125'
    provider: 'openai'
    input_token_limit: 16385 
    output_token_limit: 4096 
    tokenizer_path: gpt-3.5

  gpt-4o-2024-08-06:
    model_path: 'gpt-4o-2024-08-06'
    provider: 'openai'
    input_token_limit: 128000
    output_token_limit: 65536
    tokenizer_path: gpt-4o

  gpt-4o-mini-2024-07-18:
    model_path: 'gpt-4o-mini-2024-07-18'
    provider: 'openai'
    input_token_limit: 128000
    output_token_limit: 65536 
    tokenizer_path: gpt-4o
    
  gpt-4o-audio-preview:
    model_path: 'gpt-4o-audio-preview'
    provider: 'openai'

  gpt-o1-preview-2024-09-12:
    model_path: 'o1-preview-2024-09-12'
    provider: 'openai'
    input_token_limit: 128000
    output_token_limit: 32768 
    tokenizer_path: gpt-4

    
  o1-2024-12-17:
    model_path: 'o1-2024-12-17'
    provider: 'openai'
    input_token_limit: 128000
    output_token_limit: 32768 
    tokenizer_path: gpt-4

  o3-mini-2025-01-31:
    model_path: 'o3-mini-2025-01-31'
    provider: 'openai'
    input_token_limit: 128000
    output_token_limit: 32768 
    tokenizer_path: gpt-4


  computer-use-preview:
    model_path: computer-use-preview
    provider: openai
    mode: response

  computer-use-preview-2025-03-11:
    model_path: computer-use-preview-2025-03-11
    provider: openai
    mode: response

# Flan-t5
  flan-t5-base:
    model_path: 'google/flan-t5-base'
    tokenizer_path: 'google/flan-t5-base'
    provider: 'huggingface'

  flan-t5-small:
    model_path: 'google/flan-t5-small'
    tokenizer_path: 'google/flan-t5-small'
    provider: 'huggingface'

